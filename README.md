- üëã Hi, I‚Äôm Manohar Chekka
- üëÄ I‚Äôm Data Engineer
- üì´ Reach me at : manoharch0698@gmail.com

<!---
MC140/MC140 is a ‚ú® special ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
Semantic Models in Power BI: Best Practices, Sharing, and Governance

Introduction

Business intelligence thrives on a single source of truth for data. In Power BI, this is achieved by creating standardized semantic models (also known as Power BI datasets) that can be reused across reports . By having expert data modelers build optimized semantic models and share them, report creators can start from these models to build accurate reports, ensuring consistent data for decision-making across the organization . This document outlines the possibilities and best practices for implementing semantic models in Power BI, focusing on how to share them effectively and govern their use in a large enterprise (such as a bank) with multiple teams and data domains. It assumes a deployment pattern where data is prepared in a central warehouse (e.g. Azure Synapse) and Power BI is used as the semantic layer and reporting tool.

What is a Semantic Model in Power BI?

In Power BI, a semantic model is essentially a Power BI dataset ‚Äì a structured layer that defines tables, relationships, calculations (measures), and business logic on top of raw data. It serves as the business-friendly data model that report authors use for analysis and visualization . A well-designed semantic model centralizes important data and metrics, acting as ‚Äúone version of the truth‚Äù for all reports . When published to the Power BI service, this dataset can be reused by multiple reports and even across workspaces, enabling broad access to consistent data. Key features of a semantic model include:
‚Ä¢	Business Definitions and Logic: It encapsulates business concepts, calculations (DAX measures), and hierarchies in an intuitive format. By using business terminology and relationships that mirror the real-world entities, the model makes data easier to understand for non-technical users .
‚Ä¢	Single Subject Focus: Each semantic model typically focuses on a single subject area (or a cohesive business domain) and contains the data relevant to that topic . For example, one model might cover Retail Customer Sales while another covers Loan Portfolio. This keeps models simpler, more performant, and aligned with business boundaries ‚Äì rather than trying to cram an entire enterprise‚Äôs data into one giant model. In practice, having one giant semantic model for all data is not realistic for a large enterprise; instead, multiple domain-specific models are preferred .
‚Ä¢	Performance Optimizations: Power BI semantic models can handle large data (even terabytes) by using efficient storage modes. Frequently used data is cached in memory (Import mode) for fast queries, and DirectQuery mode can be used for huge tables or real-time needs, or a mix of both via Composite models . This means a well-built model can provide extremely responsive analytics over very large datasets by caching aggregated data and pushing detail queries to the source as needed.
‚Ä¢	Security and Permissions: The model can enforce fine-grained security, such as Row-Level Security (RLS), to ensure users see only the data they are permitted to see . All reports using the model automatically inherit these security rules. Additionally, because the model is centralized, security can be managed in one place (for example, updating an RLS filter once to apply it across all connected reports).
‚Ä¢	Consistency and Naming Conventions: Semantic models adopt consistent naming conventions for fields and measures, hide technical columns, and present data in a user-friendly way . This consistency increases trust in the data and makes it easier for analysts to find the fields they need. It‚Äôs a best practice to document major measures and assumptions within the model, so that all users and teams interpret metrics the same way .

Why Use a Central Semantic Model?

Using a central Power BI semantic model (often called a ‚Äúgolden dataset‚Äù) confers many benefits for enterprise BI:
‚Ä¢	Single Source of Truth: All reports connect to the same curated dataset, eliminating the proliferation of ‚Äúsimilar but slightly different‚Äù data models. In many self-service BI environments, different teams might create separate Power BI files with overlapping data, leading to discrepancies. A golden dataset ensures everyone is literally on the same page (or data model) when computing metrics . This avoids the governance headaches of managing multiple versions of the same logic. As one industry expert put it, having one centralized model means all reports get data from a reconciled and fully tested model ‚Äì essentially a ‚Äúgold‚Äù data model .
‚Ä¢	Reuse and Efficiency: Report authors do not need to reinvent the wheel for each new report. With a shared model, they connect to the existing dataset rather than importing data anew, enabling them to create ‚Äúthin‚Äù reports quickly . This separation of model and report accelerates development ‚Äì multiple report builders can simultaneously work on different reports all using the same underlying model. It also means any improvements to the model (new measures, corrections, performance tuning) automatically benefit all reports that use it. The modeling (DAX calculations, relationships, etc.) is done once in the central dataset and reused across dozens of reports .
‚Ä¢	Maintenance and Performance: Having one dataset instead of many reduces refresh overhead and memory footprint. Only one dataset needs to be refreshed on a schedule, instead of many duplicate datasets refreshing the same data . This uses fewer resources (storage and compute) and simplifies administration . For example, rather than ten teams refreshing ten separate copies of customer data daily, a single model refresh suffices. The Power BI service and gateway also handle fewer connections, which can improve overall performance and reliability.
‚Ä¢	Consistency in Calculations: With a central model, business calculations (KPIs, measures) are defined in one place. This ensures, for instance, that ‚ÄúTotal Deposits‚Äù or ‚ÄúNet Income‚Äù is calculated the same way in every report. It establishes a trusted definition for key metrics . Consistent semantics dramatically reduce confusion in meetings (no more ‚Äúwhich report is correct?‚Äù debates) and increase confidence in the data. All reports pulling from the model inherently align with the official business logic.
‚Ä¢	Cross-Team Sharing: A well-designed semantic model can be shared across multiple workspaces and teams, not just within a single team‚Äôs workspace. Power BI supports datasets being used by reports in other workspaces (with proper permissions) ‚Äì meaning a model built by, say, the central BI team can power reports created in various departmental workspaces . This encourages a hub-and-spoke architecture: a small number of certified ‚Äúhub‚Äù datasets feeding many ‚Äúspoke‚Äù reports in different areas of the business. It also minimizes siloed analytics.
‚Ä¢	Empowering Self-Service (with Governance): By providing business units with a ready-to-use semantic model, analysts and report developers (even those who do not have direct access to raw databases or who lack extensive data modeling skills) can create their own reports and dashboards on top of the trusted data. They can even add custom calculations in their individual reports for ad-hoc needs without altering the central model, enabling flexibility . This drives adoption of self-service BI in a governed way ‚Äì business users explore data and build insights, but within the guardrails of an official dataset. It strikes a balance between agility and control.

Thin Reports: It‚Äôs worth elaborating on the concept of ‚Äúthin reports.‚Äù A thin report is a Power BI report that does not have its own dataset; it connects live to an existing semantic model. From a user perspective, building a thin report provides the same interactive experience (you can create visuals, filters, and even add local measures), but you won‚Äôt see the raw data tables in Power BI Desktop‚Äôs Data view ‚Äì because the data resides in the central model, not embedded in the report . Thin reports are lightweight and quick to create. If the underlying dataset is updated or corrected, thin reports instantly reflect those changes. This approach greatly simplifies version control and lifecycle management: the heavy lifting is in the dataset, and reports become interchangeable presentation layers that can be iterated independently.

Semantic Model Architecture and Sharing Options
Figure 1: Example Power BI architecture with a shared semantic model. In this layered deployment, data from source systems (e.g. a data warehouse in Synapse or dataflows) is loaded and prepared, then stored in a central dataset (semantic model) in Power BI. This shared model (middle) publishes to a Premium capacity and contains all business calculations and relationships. Multiple thin reports (right) connect to this model via live connection, allowing different teams to create their own reports without duplicating the data model . Each thin report can reside in the same workspace as the dataset or even in different workspaces, thanks to cross-workspace dataset sharing capabilities. This architecture decouples the data preparation layer from the modeling layer and from the visualization layer, improving scalability and multi-developer collaboration .

One Model vs. Multiple Models: Given the size and complexity of banking data, a single monolithic model for all of the organization‚Äôs data is neither feasible nor desirable. Power BI semantic models ‚Äúcommonly focus on a single subject area, and are often widely shared.‚Äù In practice, this means designing a semantic model for each major domain or line of business (or even multiple models per LoB if needed). For example, Retail Banking data might be split into one model for Customer Transactions and another for Product Portfolio performance. Each model remains a manageable size and scope, and can be optimized and refreshed independently.

The decision on how to partition models should consider:
‚Ä¢	Data Volume and Complexity: Extremely large datasets (fact tables with billions of rows) might merit their own model, possibly using techniques like incremental refresh and aggregations to handle the load. If two subject areas share dimensions but one has vastly more data, separating them can improve refresh times and query performance.
‚Ä¢	Use Cases and Audience: Models should align with user communities and report needs. If one department never needs data from another, there is little benefit in a single combined model. Separate models allow each to be tailored (e.g., certain measures or RLS rules per department) without interfering with others.
‚Ä¢	Security Boundaries: Sometimes data must be siloed for compliance ‚Äì for example, HR or confidential finance data might be kept in a distinct model with restricted access. While RLS can secure rows within a model, maintaining completely separate models adds an extra layer of isolation when necessary.
‚Ä¢	Model Size Limits: Remember that if using Power BI Premium capacities, the model must fit in memory. Premium capacities (or Fabric capacities) have memory limits (for example, a P3 allows ~25GB per dataset on an F64 SKU). A huge ‚Äúone-size-for-all‚Äù model could hit these limits . Also, during refresh, the service needs to load a second copy of the model in memory, so a best practice is to keep model size under ~ half the capacity memory to leave headroom . Smaller focused models are easier to keep within bounds.

That said, Power BI does allow advanced scenarios like composite models across Power BI datasets (sometimes called chained datasets or DirectQuery for PBI datasets). This means one report could effectively pull from multiple semantic models. For instance, a power user could build a small composite model that combines a certified corporate dataset (perhaps customer demographics) with another dataset or Excel data for their specific use case . This is powerful for flexibility, but it adds complexity and potential performance considerations. Our general recommendation is to provide as much of the needed data in a well-designed core model as possible, to minimize the need for report authors to join data across models. If cross-domain analysis is frequently needed, it might indicate those domains should be brought together in an upstream data warehouse table or a broader semantic model if performance allows.

Cross-Workspace Sharing: A big advantage of Power BI‚Äôs semantic model architecture is that datasets can be shared across workspaces. For example, you might publish the central datasets in a dedicated ‚ÄúDatasets‚Äù workspace on the Premium capacity. Report developers in various LoB workspaces can then connect to those datasets from their own workspace. Power BI will show these as external datasets (linked) in the workspace‚Äôs lineage view, indicating the report is sourcing from another workspace‚Äôs dataset . Under the hood, it‚Äôs just a reference ‚Äì all the data stays in the original dataset; reports simply query it. An external (linked) dataset is not a copy, but essentially a pointer to the single source .

To enable this scenario, an admin setting ‚ÄúUse semantic models across workspaces‚Äù must be on (this is usually enabled by default in modern Power BI tenants) . With that in place, we can architect a workspace strategy such as: one workspace for curated datasets (with tight access control), and separate workspaces for each business unit‚Äôs reports. This isolation can help in governance (only the BI team manages the dataset workspace, ensuring quality), while still empowering distributed teams to create reports on the data. It also aligns with the bank‚Äôs setup of multiple capacities ‚Äì for instance, the central dataset might live on a production capacity, while some thin report workspaces could be on separate capacities (as long as they have access to the dataset‚Äôs capacity or the capacity is shared).

Build Permissions: Sharing a semantic model with others doesn‚Äôt mean making everyone an owner. Power BI has a specific permission called Build permission that governs who can create new content using a dataset. A user with Build permission (and at least Viewer access to the workspace containing the dataset) can connect to the dataset from Power BI Desktop or Excel and build their own reports, but they cannot edit or replace the dataset itself . In our current deployment, only about 5‚Äì10% of users (publishers) have Contributor access in workspaces, while the majority are Viewers. By granting Build permission to those viewer-role developers on a given dataset, we allow them to leverage the semantic model for new reports without elevating them to full contributors. This is a key governance mechanism: the data model remains centrally managed, yet it‚Äôs widely usable. Microsoft‚Äôs documentation states that semantic model creators can use Build permission to control who in the organization can build new content on the model . For our scenario, we will grant Build rights to approved analyst groups via Azure AD security groups ‚Äì aligning with how data source access is managed (e.g., Synapse access via AD group membership). These analysts then create reports (likely in a controlled environment like the secure VDI, if direct data access is needed to verify something), and the publishing can still be done by the authorized publishers if required.

Publishing Workflow: Given our bank‚Äôs deployment pattern, the development workflow will slightly adjust for shared models: The data team (BI developers) will create and update the semantic models (Power BI datasets) using Power BI Desktop within the secure VDI environment (to connect to Synapse). They can publish the dataset (via a .pbix or .pbit through the source control process) to the designated workspace on the Live capacity (production environment) using a publisher account. Once published, the dataset‚Äôs gateway connection to Synapse is configured (this uses our on-premises data gateway or VNet gateway to allow the cloud service to securely query the Synapse data, abiding by the rule that direct connectivity is restricted to the network). At this point, the dataset is live and accessible. Business analytics users who have Build permission can now connect to this dataset from Power BI Desktop (potentially even outside the VDI, since they are not directly hitting the database but the Power BI service ‚Äì all queries go via the service/gateway). They create their report visuals and analysis using the model. When ready, either they hand off the report file to a publisher or, if we permit it, they publish the report to their team‚Äôs workspace (the report will show up as connected to the external dataset). In either case, the new report is essentially a thin layer pointing to the certified dataset. This process maintains our existing security (since raw data access is still through the gateway) and governance (only a few can publish to production workspaces), while extending the ability to do self-service reporting on trusted data.

Best Practices for Semantic Model Design in Power BI

Designing the semantic model correctly is crucial to reaping the benefits. Here are key best practices we recommend, tailored to our current patterns and Power BI:
‚Ä¢	Model Schema ‚Äì Star Schema Approach: Organize your tables into a star schema (fact and dimension tables). This is the most performant and user-friendly design for Power BI. Remove any unnecessary tables or columns ‚Äì keep the model as simple and lean as possible . A clean model avoids confusion and runs faster. For example, rather than one huge wide table, split data into facts (transactions, balances, etc.) and dimensions (customers, products, time), which also makes applying RLS easier on dimensions.
‚Ä¢	Clear, Consistent Naming: Use business terms for table and field names (rename them from cryptic source names). Ensure names of measures and columns are consistent and unambiguous across models . If multiple models share common dimensions (like Date or Customer), use the same naming conventions so that reports built on different models still use familiar terminology. Consistency aids user adoption and reduces errors. We have internal data definitions for metrics ‚Äì those should be reflected exactly in the semantic model‚Äôs measure names and descriptions.
‚Ä¢	Hide Technical Fields: Hide any ID fields or keys that users don‚Äôt need to see for analysis. Expose only the fields that make sense for reporting. This declutters the field list and guides report authors to the right data. For example, a surrogate key or GUID should be hidden if a human-readable Customer Name is available.
‚Ä¢	Robust Measures and Calculations: Invest in creating measures for the core metrics (KPIs) that the business uses, and validate them with business stakeholders. This includes year-to-date, growth percentages, ranking metrics, etc., as needed. Ensure consistent metric definitions ‚Äì the semantic model should be the single source of truth for how a metric is calculated . If a measure is complex or its logic is not obvious, document it (in the measure description or a design document) so that everyone understands what it represents.
‚Ä¢	Performance Optimization: Limit the data in the model to what is necessary for reporting. Whenever possible, use Import mode with scheduled refresh for better query performance, and leverage incremental refresh for large fact tables so that refreshes are efficient (processing only new/changed data). Use aggregations for very large datasets ‚Äì e.g., pre-aggregating monthly totals if detailed transactions are too large. If a dataset must be DirectQuery due to size, ensure the underlying Synapse tables are optimized (indexed, partitioned) and consider enabling query caching or dual storage mode for frequently accessed dimensions. Also, avoid overly complex DAX measures that could slow down queries; pre-calculate in ETL or in the model where feasible. Regularly monitor the dataset‚Äôs performance in the Power BI Performance Analyzer and usage metrics.
‚Ä¢	Data Quality and Refresh: The semantic model is only as good as the data within it. Continue our practice of rigorous data cleaning in the Synapse stage (or use Power Query transformations in the dataset if needed for final tweaks). Set up refresh schedules appropriate to the data latency requirements (e.g., nightly or hourly refresh). Ensure the refresh happens after the upstream Synapse ETL is complete. It‚Äôs good practice to automate data quality checks ‚Äì for instance, row counts or total sum validations ‚Äì either in Synapse or post-refresh, to catch any anomalies. Power BI can send alerts or use Dataflows for staging if needed, but since we rely on the warehouse, we should do QA there.
‚Ä¢	Testing and Validation: Before certifying a semantic model for wide use, it should be thoroughly tested. This means validating that measures tie out to known correct values (from Excel or source system reports), verifying relationships and filters work as expected, and checking performance on representative queries. In our deployment, a model can be tested in a Staging workspace (on the staging capacity) by power users, then promoted to the Live workspace. Every change to the model (via our GitHub version control for .pbit files) should trigger re-testing of key metrics. A best practice is to have a set of test reports or scenarios that ensure accuracy after any model update .
‚Ä¢	Documentation: Keep a data dictionary or documentation of the semantic model. At minimum, use the built-in description fields in Power BI Desktop for tables and measures to provide explanations. This is important in a shared model because dozens of users might build reports on it; they need to understand what each field means and how measures are calculated . We can generate a document from the model (for example, using the Power BI Documenter tool or Tabular Editor) that lists all measures, their formulas, and sources. This should be updated whenever the model changes.
‚Ä¢	Row-Level Security (RLS): Implement RLS roles if the model contains data that should be restricted by user. For instance, if a semantic model includes multiple regions‚Äô data but region managers should only see their region, use RLS filters on the region field. Since our environment is highly sensitive (bank data), we must ensure any model that contains customer-level or account-level detail has the appropriate security. RLS can be defined in Power BI Desktop (Modeling > Manage Roles) and uses DAX filters (e.g., [Region] = "East" for an ‚ÄúEast Region‚Äù role). After publishing, assign Azure AD groups to these roles in the dataset‚Äôs security settings. RLS, combined with the limited distribution of build permissions, helps maintain compliance (only authorized viewers can even connect to the model, and even then they see only allowed slice). Note that RLS does not apply to users with Admin or Contributor roles on the workspace, so governance of roles is critical.
‚Ä¢	Source Control and Deployment: Continue using our practice of treating the Power BI model file (.pbix/.pbit) as code. Developers should check in changes to the GitHub repository, and we can consider using deployment pipelines or at least manual promotion steps from dev to prod to ensure quality. This provides an audit trail of changes to the model (e.g., if a measure formula was altered, we know when and why). Additionally, consider using tools like Tabular Editor for making bulk changes or applying best practice analyzers to the model before release.

Sharing and Collaboration Practices

One of the main goals is to allow more analysts (who are currently only viewers in Power BI workspaces) to leverage these semantic models for their own report creation. Below are the ways we enable sharing while preserving governance:
‚Ä¢	Certified Datasets: We will formally endorse the semantic models. Power BI allows datasets to be promoted or certified. Certification is the highest endorsement, meant for rigorously validated datasets that serve as authoritative sources. Only Power BI admins or delegated approvers can certify a dataset. We will certify our golden datasets so that users know they are official and trustworthy . In the Power BI Data Hub or ‚ÄúGet Data > Power BI Datasets‚Äù interface, certified datasets appear with a special badge and are featured at the top, making them easy to discover. This helps prevent the confusion of multiple datasets ‚Äì users can clearly identify which semantic model to use (for example, ‚ÄúSales & Revenue Model (Certified)‚Äù). According to Power BI guidance, this labeling helps clarify which datasets are tested and reliable, and it supports a ‚ÄúGold/Silver/Bronze‚Äù content strategy for data assets . Our governance policy will be that only BI team-created models can be certified (self-service models by individual users, if any, might be left as unendorsed or promoted at most).
‚Ä¢	Access Control via Roles: We will maintain the two-level access in workspaces: Contributor (publishers) and Viewer (consumers). However, for viewers to create new reports on a dataset, they need Build permission on that dataset. Instead of elevating their workspace role, we grant Build permission explicitly. This can be done by dataset owners in the dataset settings (or via the Share Dataset dialog, which lets you give build access to specific users or ‚Äì better ‚Äì to an Azure AD security group). The Microsoft documentation emphasizes that Build permission allows users to create new content from the dataset, even outside of Power BI (e.g., Excel‚Äôs Analyze in Excel feature) . We will likely create an AD group for each semantic model (or each subject area) and assign our analysts to those groups, then give the group Build access. This way, if personnel change, we just update the group membership. The Viewer + Build combination allows a user to use the dataset without exposing any edit/delete capabilities. It‚Äôs a least-privilege approach consistent with our security requirements.
‚Ä¢	Communicating Availability: All business analytics teams need to know these shared semantic models exist and what‚Äôs in them. We will publish documentation or an internal catalog entry for each semantic model, listing its contents, refresh frequency, responsible owners, and usage guidelines. Power BI‚Äôs Data Hub is a useful interface ‚Äì it shows all datasets a user has access to. Thanks to endorsement, our certified models will show up prominently. We might also consider using the new ‚ÄúSemantic model catalog‚Äù feature in the service, which lets you open a workspace and see semantic models from other workspaces that you have access to . Training sessions or brown-bags can help analysts learn how to connect to and use these models (for example, demonstrating how to use Power BI Desktop‚Äôs Get Data -> Power BI Datasets to connect to a live model ).
‚Ä¢	Thin Report Workspaces: If multiple teams will build reports on the same dataset, a question arises: should they publish those reports into the dataset‚Äôs workspace or their own? We have options here:
o	Single workspace: Keep dataset and reports together. If most report creators are view-only in that workspace, they could develop reports locally and then hand them to a Contributor for publishing. This is our current workflow. The benefit is simplicity (everything in one place) but it might bottleneck publishing through a few individuals.
o	Separate workspaces: The dataset lives in a central workspace (managed by IT/BI), while each business team has their own workspace for their reports (where they do have contributor access for publishing reports). They connect to the central dataset (which appears as an external dataset link in their workspace). This gives teams autonomy to manage their report lifecycle and content (including creating apps for distribution) without needing to go through IT for every report change, yet they‚Äôre all using the governed data model . The trade-off is that we need to manage permissions carefully ‚Äì the central dataset workspace must include all users (or groups) that need access (likely as viewers with build). This also means if one team‚Äôs report needs to be used by another team, cross-workspace app access might be needed or we consider a unified app catalog.
We can adopt a hybrid: for core corporate dashboards, BI can publish the report in the central workspace using the central dataset. For ad-hoc or team-specific reports, allow them to be in team workspaces using the central dataset. We will ensure our Power BI tenant settings allow cross-workspace dataset usage (governed by the admin setting mentioned earlier) and likely disable any broad ‚ÄúPublish to web‚Äù on reports that rely on these models (since that could leak data ‚Äì indeed, Power BI by design blocks Publish-to-Web for reports on shared datasets , which aligns with our security stance).
‚Ä¢	Versioning and Impact: One concern with shared datasets is that a change could affect many reports. To manage this, our governance will include a process for dataset updates: for example, adding new measures is usually safe (reports won‚Äôt use them until they choose to), but changing an existing measure‚Äôs definition or removing a field could break reports. Thus, any breaking changes should be communicated and ideally avoided (instead, deprecate slowly or create new fields). Using the deployment pipeline‚Äôs impact analysis feature can list which reports use a dataset, which helps in assessing the blast radius of a change. We will likely maintain backward compatibility in the model or coordinate updates with report creators if a major change is needed.
‚Ä¢	Collaboration and Feedback: Encourage report creators to request new measures or attributes in the semantic model when they find a gap. Since not everyone can edit the model, there should be a clear channel to funnel these requests to the BI data team. For instance, if many analysts are creating a similar calculated measure in their thin reports, that‚Äôs a strong candidate to incorporate into the central model (so it benefits all). We might set up a monthly review of user feedback on the models. This way the semantic model evolves with business needs, but in a controlled, governed manner (with proper validation for each addition).

Governance and Security Considerations

Operating shared semantic models in a highly regulated environment like banking requires strict governance standards, many of which we already have in place. Below are the key governance measures and any new ones introduced with semantic model sharing:
‚Ä¢	Limited Contributors (Data Model Ownership): We will continue the practice that only a small, trusted group (5-10% of users) have Contributor (edit/publish) rights on workspaces. This means the semantic model can only be created or modified by this group (primarily the central data team). This prevents ad hoc or unverified datasets from proliferating. All semantic models will go through a review and approval before being published or certified. Essentially, treat the semantic model as a governed data product ‚Äì with clear ownership, stewardship, and quality checks.
‚Ä¢	Role Separation: The separation between data preparation (ETL in Synapse), data modeling (Power BI dataset), and report creation (thin reports) inherently provides checks and balances. Data engineers ensure data is correct and loaded, BI modelers ensure the logic is correct, and report designers create visuals knowing they cannot accidentally change underlying data. Each role focuses on their expertise, which is a benefit of the layered architecture . We will formalize this in our CoE (Center of Excellence) guidelines: who is allowed to do what in each layer.
‚Ä¢	Security of Data Connections: In our current setup, direct queries to the database are only allowed within the secure network (VDI). With a published dataset, queries will flow via the on-premises data gateway which runs inside our network. We must secure that gateway ‚Äì ensuring it‚Äôs updated, only allowed to connect to approved data sources, and that only authorized datasets use it. Gateway admin settings will restrict who can create new connections. For the semantic models, the gateway mapping to Synapse uses a service account or managed identity with read access to only the necessary data. This prevents any misuse of the dataset to query unintended tables. Additionally, because the data is cached in the Power BI service (for Import mode), we need to ensure the Power BI service is considered part of our trusted landscape (we already have necessary cloud security in place via Azure AD, conditional access, etc.). All access to the Power BI service is authenticated through our enterprise AD, and we have logging (via audit logs) of who accessed which dataset. These audit logs should be periodically reviewed for unusual access patterns.
‚Ä¢	Data Classification and Sensitivity Labels: We should apply sensitivity labels in Power BI to our datasets (a feature that integrates with Microsoft Purview Information Protection). For example, label the datasets as ‚ÄúConfidential ‚Äì Internal‚Äù or ‚ÄúHighly Confidential‚Äù as appropriate. This will put visual markings on reports and enable downstream controls (like preventing exporting data from a highly confidential dataset if desired). It‚Äôs another layer of governance to ensure users treat the data appropriately.
‚Ä¢	Endorsement and Cataloging: As mentioned, certification of datasets will be done through the Power BI endorsement mechanism. We may maintain a central catalog or inventory of all certified semantic models, including their purpose, owners, and last refresh date. This catalog can be part of our BI portal or SharePoint site for the Analytics Community. It ensures everyone knows where to find trusted data. The endorsement labels (Certified/Promoted) are visible in the Power BI interface, which reduces the chance of someone using the wrong dataset inadvertently . For example, if an analyst sees two ‚ÄúFinance Model‚Äù datasets, one certified and one not, governance guidance would be to use the certified one unless there‚Äôs a very specific reason not to.
‚Ä¢	Governance Committee & Change Management: We might establish a small BI governance council that oversees changes to core semantic models. This group would review any proposed major changes or new models for alignment with overall standards. Governance standards include verifying that a new semantic model does not duplicate an existing one (to avoid silos), and that it adheres to naming conventions, performance guidelines, and security requirements. Essentially, treat changes to these golden datasets with the same rigor as changes to a production database. Using our source control (Git) and possibly Power BI deployment pipelines, changes can be tested in a lower environment or a separate workspace first.
‚Ä¢	Monitoring and Auditing: Leverage Power BI‚Äôs monitoring tools to govern usage. The admin portal and Premium capacity metrics can show dataset refresh status, failures, and performance. We will set up alerts on refresh failures so the data team can respond quickly (nothing hurts trust more than stale data going unnoticed). Also, usage metrics reports can tell us how often each dataset is used and by how many reports/users. If a certified dataset is not being widely used, perhaps users are encountering issues or it needs more evangelization ‚Äì that‚Äôs something to follow up on. Conversely, if it‚Äôs heavily used, we need to ensure it‚Äôs scaled appropriately and consider if any optimization is required to handle load (e.g., scaling up capacity or using the Azure Analysis Services scale-out pattern with query replicas if needed , though in Fabric this might translate to using multiple capacity nodes behind an Azure Load Balancer for extremely heavy query loads ‚Äì likely not needed initially, but good to know). We will periodically audit who has access to each dataset (especially Build permission) to make sure it remains appropriate as people join/leave teams.
‚Ä¢	Compliance and Data Governance Alignment: We will align the semantic model governance with the bank‚Äôs broader data governance policies. For instance, if certain data is deemed sensitive (PII or financial), the semantic model should enforce any necessary aggregation or anonymization. If needed, we won‚Äôt include extremely sensitive granular data in a widely shared model unless required ‚Äì instead, include only aggregated or masked values. All content in Power BI is already under our enterprise compliance umbrella (with audit trails), but we should treat the semantic model content as an extension of the data warehouse in terms of governance oversight.

By following these governance practices, we ensure that broad sharing of semantic models does not equate to uncontrolled access, but rather controlled self-service ‚Äì users get the data they need in a secure, managed way.

Recommendations and Next Steps

Implement Domain-Level Semantic Models: We recommend creating one certified Power BI dataset (semantic model) per major data domain or line of business, rather than one giant model for all. For example, create a Customer Analytics Model, a Loans & Mortgages Model, a Payments Transactions Model, etc., each sourcing from the corresponding Synapse tables for that domain. This aligns with industry best practices of separate models for different business domains to avoid an unwieldy ‚Äúall-in-one‚Äù model . Each model can be sized and tuned appropriately. If there are common reference dimensions (like a Calendar or Branch dimension used by multiple models), ensure those are consistent (or even consider a separate small ‚Äúcommon dimensions‚Äù model if composite models are to be used, though that adds complexity).

Establish a Central BI Dataset Workspace: Set up a dedicated workspace (or use an existing one) on the production capacity to host these certified datasets. Limit direct access to this workspace (perhaps BI developers as Members and relevant data admins as Admins). This is where dataset publishing occurs. Enable large models, incremental refresh, and other settings as needed on this capacity. Ensure the gateway connections for all datasets in this workspace are configured and tested.

Grant Access via Build Permissions: For each dataset, identify the list of users or (preferably) AD groups that need to build reports on it. Grant them Viewer access to the workspace (if not already via an app or such) and Build permission on the dataset. We may use the Manage Permissions dialog of the dataset to add these. Going forward, anytime a new analyst needs access, the governance process should route through adding them to the appropriate group. Avoid giving outright download permission or making them workspace Contributors, which could bypass governance. Build permission will be the main mode for self-service.

Promote and Educate: Once the semantic models are in place, conduct a rollout: publish an internal news post or include in the BI newsletter that these new shared datasets are available. Highlight their names, what they contain, and how to connect. Perhaps create a simple demo: e.g., ‚ÄúHow to create your first thin report‚Äù showing a step-by-step to connect to a certified dataset and build a report. Emphasize the benefits: no need to worry about data prep or complex DAX ‚Äì it‚Äôs been done for you. Reinforce that these are the official data sources for reporting, in line with governance. This will drive adoption and also reduce the tendency of teams to create their own rogue Excel data exports.

Monitor Usage and Gather Feedback: In the initial months after deployment, closely monitor how the models are being used. Use the Power BI usage metrics or the Admin portal‚Äôs dataset metrics to see number of queries, etc. Also solicit feedback from the business users: Are the models meeting their needs? Any additional fields or measures frequently requested? Any confusion that needs clarifying (perhaps some naming that needs improvement)? Treat this as an iterative process ‚Äì the semantic model can be refined over time.

Maintain Alignment with Data Warehouse: Because our semantic models sit on top of the Synapse data warehouse, it‚Äôs critical to stay in sync with that source. Any changes in the warehouse (new tables, changes in logic) should be reflected in the semantic model. Establish a communication channel with the data engineering team: e.g., if a new customer segmentation field is added in Synapse, the BI team should consider adding it to the model and exposing it to users (with documentation). Conversely, insights from the BI side (like a measure definition refined in DAX) could sometimes be pushed down into the warehouse for efficiency. This ensures that the ‚Äúenterprise models‚Äù in the warehouse and the ‚ÄúBI semantic models‚Äù in Power BI remain consistent and up-to-date . Essentially, the warehouse is the single source of truth for raw data, and the Power BI semantic model is the single source of truth for analytics-ready data ‚Äì both need to work hand in hand.

Governance Review Cycle: Finally, fold the semantic model governance into our regular data governance forums. Perhaps quarterly, review all certified datasets: Are they still accurate? Do they need any updates due to regulatory changes or business rule changes? Ensure RLS roles are correct as people move around. Review access logs for any anomalies (like someone querying a lot of data they shouldn‚Äôt). Check that no one has attempted to circumvent the model by creating their own duplicate dataset with the same data ‚Äì if so, address via communication or restrictions. Reinforce the policy that official reports should use these models (maybe even mandate that any report published to the Power BI Apps for broad consumption must be built on a certified dataset, unless exception granted). This will further solidify the single source of truth approach and enhance our data culture.

In summary, adopting shared Power BI semantic models will streamline our BI environment ‚Äì providing consistency, reducing duplication, and empowering business users in a governed way. By following the best practices and governance standards outlined above, we can safely transition to this model-centric approach. The result should be faster development of reports, more reliable and unified analytics, and stronger collaboration between IT and business teams on our data. As we implement these changes, we will continuously monitor and adjust to ensure that the solution remains scalable, secure, and aligned with business needs.

With these semantic models in place, our bank will be well-positioned to leverage data as a true enterprise asset, enabling both high-level strategic dashboards and detailed operational reports to draw from the same trusted data foundation. This is a significant step toward a mature, federated self-service BI capability within our Power BI ecosystem.  

